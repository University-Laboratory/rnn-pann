# やること

- シミュレーション波形を CSV 化して読み込み、実機での学習と比較する(BuckConverterCell のみ)
- plot 系を utils に共通化したけどどうやらそれも間違ってる可能性がありそうなので極力このファイル内で完結させる

# 結果

- BuckConverterCell の結果はまあまあ悪くないが、なぜかシミュレーションした時だけに比べると悪くなった
  - 多分、dt が一定じゃないからだと思う
- GRU の結果は結構良かった
  - 無次元化、標準化がめっちゃ良かったっぽくて、ほとんど GRU のおかげで精度がめっちゃ上がったが不本意

# 考察

- もしかしたら dt が一定じゃないのってかなりヤバめ？？
- シミュレーション波形なら一定の dt で作れるし、iL, vC ともきっちり同期できるが、実機なら dt を微妙な差だけどちょっとずれてる
  - スイッチングのタイミングも肉眼である程度同期させてるだけやし、スイッチングノイズでよくわからんし
- 結局 BuckConverterCell でもっと精度を上げないと説得力が無さすぎる
  - 本来はノイズだけを推論する GRU が BuckConverterCell の尻拭いしてるだけやん
  - なんで無理なの〜わからない
  - とりあえず移動平均取ってやるしかないかな
- スイッチング信号とか入力電圧を数値計算的にやってるのもかなりやばい気がするんだよなー
  - だってタイミング絶対合ってないもん
  - そもそもなんで実機データ取る時に iL と vC だけでいいと思ったん？じゃあその iL と vC しか学習に使ったらあかんやろ
  - u と vs も絶対取るべきじゃね？
  - vs はまあ一定だからどっちでもいいか、でも u は絶対実測に含めないとめっちゃズレるのは仕方ないな
- てかそもそも u が duty0.5 基準なら大体出力もそのくらいになっちゃうんじゃね？
